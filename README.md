# Project Lenie: Personal AI Assistant

Project Lenie, named after the enigmatic protagonist from Peter Watts' novel "Starfish,"
offers advanced solutions for collecting, managing, and searching data using
Large Language Models (LLMs).

Lenie enables users to:
* collect and manage links, allowing easy searching of accumulated references using LLM,
* download content from webpages and store it in a PostgreSQL database for later search in a private archive,
* transcribe YouTube videos and store them in a database, facilitating the search for interesting segments (given the ease of finding engaging videos compared to books or articles).

Lenie's functionalities represent an advanced integration of AI technology with users' daily needs, providing efficient data management and deeper content analysis and utilization. However, similar to the literary character who brings inevitable consequences of her existence, Lenie raises questions about the boundaries of technology and our own control over it. It is both a fascinating and daunting tool that requires a conscious approach and responsible usage to maximize benefits and minimize risks associated with the increasing role of artificial intelligence in our lives.

This is a side project. Please be aware that the code is under active refactoring and correction as I'm still learning Python and LLMs.

## Components

* Web interface for browsing database contents
* Chrome and Kiwi Browser extension
* Backend written in Python

## Supported Platforms

| Platform | Support |
|---|---|
| Windows | Chrome + extension |
| Android | Kiwi Browser + extension |
| MacOS | None |

## Differences Compared to Corporate Knowledge Bases
In corporate knowledge bases, we don't assume that we have misleading, inappropriate, or propaganda-driven articles.
Every article is considered equally valid.

When dealing with sensitive, political, or money-related topics, we may encounter:
* state propaganda (especially on geopolitical and political topics)
* party-driven thematic propaganda (anti-EU, refugees and immigrants, vaccines, etc.)
* corporate Public Relations campaigns (e.g., "it's not true that Tesla fell behind in autonomous and electric vehicle development")
* online scammers
* amateur texts posing as expert content (e.g., tutorials advising to disable all Linux security mechanisms because they're "inconvenient")
* internet trolls
* mass AI-generated content with no real value, created to gain Google search rankings

Therefore, there is a need to build a mechanism for assessing the credibility of sources (e.g., websites or YouTube videos) and authors.

It is also necessary to provide the ability to select only specific sources (from all available ones) and to explicitly cite data sources in responses.

## Challenges to Solve When Building Such a Solution

When working with corporate documents, the most common challenge is converting corporate wikis, Notion pages, or Word documents into a format suitable for LLMs.

When working with internet sources, the challenges are different:
* content is behind a paywall (the solution I use is a browser extension),
* difficulty importing data from platforms like LinkedIn, Facebook, etc. (they protect against easy content scraping),
* need to write content analyzers for pages captured by the extension to reduce costs (see below),
* quality of subtitles generated by YouTube's automatic translation,
* cost (and quality) of audio-to-text conversion

Example document sizes:
* Original HTML document, a saved copy of an article from Onet.pl: 300 KB,
* Converted to markdown format: 15 KB,
* Article text only: 3000 words,

Large language models, such as those from OpenAI, handle the analysis of an entire article page in markdown format very well, but this generates significant costs compared to analyzing just the article text.

Data sources for a personal assistant:
* SMS messages, i.e., messages up to 120 characters (Google Play has been blocking apps with SMS access for some time; you need to install a "custom" app, e.g., Make),
* emails (HTML format), several hundred words,
* PDF documents (e.g., invoices) and DOC files (e.g., job requirements),
* ebooks (hundreds of thousands of words, need to be split into chunks before embedding),
* images (e.g., photos of book pages, invoices, photos with significant content),
* WhatsApp chats, Messenger, etc.,
* calendar access,
* browsing history (access to SQLite, e.g., in Chrome),
* access to the paid Meetup API (GraphQL) to know who you might meet and who to be cautious of,
* access to paid APIs for querying the Polish National Court Register (KRS) (to know if a contact has their own company, foundation, etc.)


## Scalability and Reliability
For a single user, a PostgreSQL database with appropriate extensions is sufficient.

If we want a single user to be able to work from different devices, we need to enable
them to work with an external server running 24/7.
In that case, we must ensure:
* availability of the solution from anywhere in the world,
* security of the solution (need for security updates, DDoS protection, etc.),
* low costs,
* minimal maintenance time required.



For a larger number of users, we need to consider:
* infrastructure scaling costs (e.g., database),
* solution performance (we can add containers or go with serverless solutions and queues),
* security of data isolation for each client.

## Used Technologies
In this project, I'm using:
* Python as the server backend
* PostgreSQL as the embedding database
* React as the web interface (under development)
* HashiCorp Vault for secrets (for local and Kubernetes environments)
* AWS as the deployment platform (as I'm lazy and don't want to manage infrastructure)

I'm also preparing several deployment methods:
* Docker image (for easy application deployment)
* Kubernetes Helm (to test scalability options)
* AWS Lambda (to test the Event-Driven way of writing applications)

As I'm a big fan of AWS, you will also see deployment approaches like:
* Lambdas (to explore the Event-Driven way of writing applications like this),
* ECS (to explore a convenient way of scaling Docker images),
* EKS (to learn more about the costs of managing your own Kubernetes cluster and applications on it)


## Python Notes

### Dependencies

Dependencies are managed via [uv](https://github.com/astral-sh/uv) package manager and configured in `backend/pyproject.toml` with optional dependency groups (`docker`, `markdown`, `all`).

See [CLAUDE.md](CLAUDE.md) for detailed dependency installation commands (`make install`, `uv sync`, etc.).


## Prerequisites
Before running the Docker container with the Lenie application, make sure you have:

* Docker installed on your computer. Installation instructions can be found in the official Docker documentation.

To create a Docker image for the Lenie application, you need a Dockerfile in your project directory. Below is an example process of building the image.

1. Open a terminal in the directory where the Dockerfile is located.

2. Run the following command to build the Docker image:

```bash
docker build -t stalker-server2:latest .
```

* The `-t` flag is used to tag (name) the image, in this case stalker.
* The dot `.` at the end indicates that the Dockerfile is in the current directory.

After the build process is complete, you can run the Docker container with the newly created image by using the command described in the section Running the Container.

## Virtual Linux Machine

### Debian machine
```bash
useradd lenie-ai
mkdir /home/lenie-ai
chown lenie-ai:lenie-ai /home/lenie-ai/

apt-get install git
apt install python3.11-venv
apt install python3-pip

```

Installation of PostgreSQL database

```bash

```

```bash
python3 server.py
```

## AWS

### Pushing Image to ECR
```powershell
(Get-ECRLoginCommand -ProfileName stalker-free-developer -Region us-east-1).Password | docker login --username AWS --password-stdin ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com
```

```powershell
docker build -t lenie-ai-server .
```
```powershell
docker tag lenie-ai-server:latest ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/lenie-ai-server:latest
```

```powershell
docker push ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/lenie-ai-server:latest
```


## Accessing the Application

After starting the application or container, you can access the Lenie application by going to http://localhost:5000 in your web browser.

### Docker

#### Preparing Local Environment

Install the Vault binary from: https://developer.hashicorp.com/vault/install

```bash
docker volume create vault_secrets_dev
docker volume create vault_logs_dev

 docker run -d --name=vault_dev --cap-add=IPC_LOCK -e 'VAULT_LOCAL_CONFIG={"storage": {"file": {"path":
 "/vault/file"}}, "listener": [{"tcp": { "address": "0.0.0.0:8200", "tls_disable": true}}], "default_lease_ttl": "168h", "max_lease_ttl":
"720h", "ui": true}' -v vault_secrets_dev:/vault/file -v vault_logs_dev:/vault/logs -p 8200:8200 hashicorp/vault server
```

```bash
docker pull pgvector/pgvector:pg17
```

```bash
docker run -d --name lenie-ai-db -e POSTGRES_PASSWORD=postgres -p 5432:5432 pgvector/pgvector:pg17
```

```sql
CREATE EXTENSION vector
```


#### Running the Application

Running from a local image:
```bash
docker run --rm --env-file .env -p 5000:5000 --name lenie-ai-server -d lenie-ai-server:latest
```

Running from a remote image:

```powershell
docker run --rm --env-file .env -p 5000:5000 --name lenie-ai-server -d lenieai/lenie-ai-server:latest
```

### Docker Compose

```shell
docker-compose.exe create
docker-compose.exe start
```

## Working with the API

You can send an example API request from the command line:

```shell

curl -X POST https://pir31ejsf2.execute-api.us-east-1.amazonaws.com/v1/url_add \
     -H "Content-Type: application/json" \
     -H "x-api-key: XXXX" \
     -d '{
           "url": "https://tech.wp.pl/ukrainski-system-delta-zintegrowany-z-polskim-topazem-zadaje-rosjanom-wielkie-straty,7066814570990208a",
           "type": "webpage",
           "note": "Interesting integration with the Polish battlefield imaging system",
           "text": "HTML of the page from the given URL"
         }'
```

If port forwarding is enabled, you can use this to validate your API request:

```
curl -H "x-api-key: XXX" -X GET "http://localhost:5000/website_list?type=ALL&
document_state=ALL&search_in_document="
```


## Code Quality & Security

For linting, formatting, and testing commands, see [CLAUDE.md](CLAUDE.md).

### Security Scanning
All security tools are run via `uvx` (uv tool runner) to avoid adding heavy dependencies to the project venv.

```bash
make security        # Run semgrep static analysis
make security-deps   # Check dependencies for vulnerabilities (pip-audit)
make security-bandit # Run bandit Python security linter
make security-safety # Check dependencies with safety
make security-all    # Run all security checks
```

| Tool | Purpose |
|------|---------|
| Semgrep | Static code analysis, security vulnerabilities |
| pip-audit | Dependency vulnerability scanning (PyPI advisory DB) |
| Bandit | Python-specific security linter |
| Safety | Dependency vulnerability check (requires free account) |

### Pre-commit Hooks (TruffleHog)
Pre-commit hooks include TruffleHog for secret detection. See `.pre-commit-config.yaml`.


# Planned Improvements
* Add a checker to verify that no Lambda uses AWS Lambda Layers anymore


## Why Do We Need Our Own LLM?
So far, available LLMs operate in English or implicitly translate to English, losing context or meaning.

Let's translate two texts into English:

Sasiad wyszedl z psem o 6 rano.
(The neighbor went out with a dog at 6 AM.)

And:

Psy przyszly po sasiada o 6 rano.
(The "dogs" came for the neighbor at 6 AM.)

As Poles, we perfectly understand the difference between an animal and the slang term for police officers ("psy" literally means "dogs" but is slang for "cops"), but you need to know the cultural context.

Now we have Bielik (https://bielik.ai), which perfectly understands the magic of this sentence:

![img.png](bielik_psy_pl.png)

You can use Bielik on [CloudFerro.com](https://sherlock.cloudferro.com/#pricing)
