# Project Lenie: Personal AI Assistant

Project Lenie, named after the enigmatic protagonist from Peter Watts' novel "Starfish,"
offers advanced solutions for collecting, managing, and searching data using
Large Language Models (LLMs).

Lenie enables users to:
* collect and manage links, allowing easy searching of accumulated references using LLM,
* download content from webpages and store it in a PostgreSQL database for later search in a private archive,
* transcribe YouTube videos and store them in a database, facilitating the search for interesting segments (given the ease of finding engaging videos compared to books or articles).

Lenie's functionalities represent an advanced integration of AI technology with users' daily needs, providing efficient data management and deeper content analysis and utilization. However, similar to the literary character who brings inevitable consequences of her existence, Lenie raises questions about the boundaries of technology and our own control over it. It is both a fascinating and daunting tool that requires a conscious approach and responsible usage to maximize benefits and minimize risks associated with the increasing role of artificial intelligence in our lives.

This is a side project. Please be aware that the code is under active refactoring and correction as I'm still learning Python and LLMs.

## Components

* Web interface for browsing database contents
* Chrome and Kiwi Browser extension
* Backend written in Python

## Supported Platforms

| Platform | Support |
|---|---|
| Windows | Chrome + extension |
| Android | Kiwi Browser + extension |
| MacOS | None |

## Differences Compared to Corporate Knowledge Bases
In corporate knowledge bases, we don't assume that we have misleading, inappropriate, or propaganda-driven articles.
Every article is considered equally valid.

When dealing with sensitive, political, or money-related topics, we may encounter:
* state propaganda (especially on geopolitical and political topics)
* party-driven thematic propaganda (anti-EU, refugees and immigrants, vaccines, etc.)
* corporate Public Relations campaigns (e.g., "it's not true that Tesla fell behind in autonomous and electric vehicle development")
* online scammers
* amateur texts posing as expert content (e.g., tutorials advising to disable all Linux security mechanisms because they're "inconvenient")
* internet trolls
* mass AI-generated content with no real value, created to gain Google search rankings

Therefore, there is a need to build a mechanism for assessing the credibility of sources (e.g., websites or YouTube videos) and authors.

It is also necessary to provide the ability to select only specific sources (from all available ones) and to explicitly cite data sources in responses.

## Challenges to Solve When Building Such a Solution

When working with corporate documents, the most common challenge is converting corporate wikis, Notion pages, or Word documents into a format suitable for LLMs.

When working with internet sources, the challenges are different:
* content is behind a paywall (the solution I use is a browser extension),
* difficulty importing data from platforms like LinkedIn, Facebook, etc. (they protect against easy content scraping),
* need to write content analyzers for pages captured by the extension to reduce costs (see below),
* quality of subtitles generated by YouTube's automatic translation,
* cost (and quality) of audio-to-text conversion

Example document sizes:
* Original HTML document, a saved copy of an article from Onet.pl: 300 KB,
* Converted to markdown format: 15 KB,
* Article text only: 3000 words,

Large language models, such as those from OpenAI, handle the analysis of an entire article page in markdown format very well, but this generates significant costs compared to analyzing just the article text.

Data sources for a personal assistant:
* SMS messages, i.e., messages up to 120 characters (Google Play has been blocking apps with SMS access for some time; you need to install a "custom" app, e.g., Make),
* emails (HTML format), several hundred words,
* PDF documents (e.g., invoices) and DOC files (e.g., job requirements),
* ebooks (hundreds of thousands of words, need to be split into chunks before embedding),
* images (e.g., photos of book pages, invoices, photos with significant content),
* WhatsApp chats, Messenger, etc.,
* calendar access,
* browsing history (access to SQLite, e.g., in Chrome),
* access to the paid Meetup API (GraphQL) to know who you might meet and who to be cautious of,
* access to paid APIs for querying the Polish National Court Register (KRS) (to know if a contact has their own company, foundation, etc.)


## Scalability and Reliability
For a single user, a PostgreSQL database with appropriate extensions is sufficient.

If we want a single user to be able to work from different devices, we need to enable
them to work with an external server running 24/7.
In that case, we must ensure:
* availability of the solution from anywhere in the world,
* security of the solution (need for security updates, DDoS protection, etc.),
* low costs,
* minimal maintenance time required.



For a larger number of users, we need to consider:
* infrastructure scaling costs (e.g., database),
* solution performance (we can add containers or go with serverless solutions and queues),
* security of data isolation for each client.

## Used Technologies
In this project, I'm using:
* Python as the server backend
* PostgreSQL as the embedding database
* React as the web interface (under development)
* HashiCorp Vault for secrets (for local and Kubernetes environments)
* AWS as the deployment platform (as I'm lazy and don't want to manage infrastructure)

I'm also preparing several deployment methods:
* Docker image (for easy application deployment)
* Kubernetes Helm (to test scalability options)
* AWS Lambda (to test the Event-Driven way of writing applications)

As I'm a big fan of AWS, you will also see deployment approaches like:
* Lambdas (to explore the Event-Driven way of writing applications like this),
* ECS (to explore a convenient way of scaling Docker images),
* EKS (to learn more about the costs of managing your own Kubernetes cluster and applications on it)

## Services That Can Be Used to Get Data

| Service name | Provider   | Description | Link |
|-------------|------------|---|------|
| Textract    | AWS        | PDF to text | https://aws.amazon.com/textract/     |
| AssemblyAI  | AssemblyAI | Speech to text ($0.12 per hour) | https://www.assemblyai.com/ |

## Documentation

| Document | Description |
|----------|-------------|
| [CLAUDE.md](CLAUDE.md) | Full architecture reference |
| [docs/Python_Dependencies.md](docs/Python_Dependencies.md) | Dependency management with uv |
| [docs/Docker_Local.md](docs/Docker_Local.md) | Docker development and deployment |
| [docs/VM_Setup.md](docs/VM_Setup.md) | Virtual machine setup |
| [docs/AWS_Infrastructure.md](docs/AWS_Infrastructure.md) | AWS infrastructure |
| [docs/Code_Quality.md](docs/Code_Quality.md) | Linting and security scanning |
| [docs/API_Usage.md](docs/API_Usage.md) | API request examples |
| [docs/CI_CD.md](docs/CI_CD.md) | CI/CD pipelines |

# Planned Improvements
* Add a checker to verify that no Lambda uses AWS Lambda Layers anymore


## Why Do We Need Our Own LLM?
So far, available LLMs operate in English or implicitly translate to English, losing context or meaning.

Let's translate two texts into English:

Sasiad wyszedl z psem o 6 rano.
(The neighbor went out with a dog at 6 AM.)

And:

Psy przyszly po sasiada o 6 rano.
(The "dogs" came for the neighbor at 6 AM.)

As Poles, we perfectly understand the difference between an animal and the slang term for police officers ("psy" literally means "dogs" but is slang for "cops"), but you need to know the cultural context.

Now we have Bielik (https://bielik.ai), which perfectly understands the magic of this sentence:

![img.png](bielik_psy_pl.png)

You can use Bielik on [CloudFerro.com](https://sherlock.cloudferro.com/#pricing)
